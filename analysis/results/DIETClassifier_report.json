{
  "greenhouse_gas": {
    "precision": 0.8947368421052632,
    "recall": 0.8947368421052632,
    "f1-score": 0.8947368421052632,
    "support": 19,
    "confused_with": {}
  },
  "influence_type_on_climate": {
    "precision": 1.0,
    "recall": 0.7647058823529411,
    "f1-score": 0.8666666666666666,
    "support": 17,
    "confused_with": {}
  },
  "consequence_category": {
    "precision": 0.8888888888888888,
    "recall": 0.5714285714285714,
    "f1-score": 0.6956521739130435,
    "support": 14,
    "confused_with": {
      "greenhouse_gas": 1
    }
  },
  "meat_type": {
    "precision": 0.9705882352941176,
    "recall": 0.9428571428571428,
    "f1-score": 0.9565217391304348,
    "support": 35,
    "confused_with": {}
  },
  "topic": {
    "precision": 0.7647058823529411,
    "recall": 0.5909090909090909,
    "f1-score": 0.6666666666666667,
    "support": 22,
    "confused_with": {
      "consequence_category": 1
    }
  },
  "micro avg": {
    "precision": 0.9130434782608695,
    "recall": 0.7850467289719626,
    "f1-score": 0.8442211055276382,
    "support": 107
  },
  "macro avg": {
    "precision": 0.9037839697282422,
    "recall": 0.752927505930602,
    "f1-score": 0.8160488176964149,
    "support": 107
  },
  "weighted avg": {
    "precision": 0.9087716083318063,
    "recall": 0.7850467289719626,
    "f1-score": 0.8375457131247461,
    "support": 107
  },
  "accuracy": 0.9918585064570467
}